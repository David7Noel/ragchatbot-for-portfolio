üí¨ Einfacher Chatbot mit Ollama, FastAPI und Gradio ‚Äì Lokale LLM-Inferenz
Willkommen zu meinem neuesten Projekt: Ein minimalistischer Full-Stack-Chatbot, der ein gro√ües Sprachmodell (LLM) komplett lokal ausf√ºhrt! In diesem Video zeige ich dir, wie dieser Chatbot funktioniert und welche Technologien dahinterstecken.

‚ú® Funktionen
Lokale LLM-Inferenz: Nutzt Ollama, um gro√üe Sprachmodelle (LLMs) wie Gemma oder Llama 2 direkt auf deinem Computer auszuf√ºhren ‚Äì keine Cloud-APIs, keine Kosten, maximaler Datenschutz!

Full-Stack-Architektur:

Backend: Robuste RESTful API mit FastAPI.

Frontend: Intuitive Web-Oberfl√§che mit Gradio.

Einfache Interaktion: Eine benutzerfreundliche Chat-Schnittstelle zum Stellen von Fragen an das LLM.

Kostenfrei & Datenschutzfreundlich: Da das LLM lokal l√§uft, fallen keine API-Kosten an und deine Daten verlassen deinen Rechner nicht.

## üé¨ Demo

Schau dir eine kurze Video-Demo des Chatbots in Aktion an:

[![Chatbot Demo Video](https://img.youtube.com/vi/2egz-IzPhmo/0.jpg)](https://www.youtube.com/watch?v=2egz-IzPhmo)
*(Hinweis: Das Vorschaubild wird automatisch von YouTube generiert. Der Link f√ºhrt direkt zum Video.)*

üõ†Ô∏è Lokale Einrichtung
Befolge diese Schritte, um den Chatbot auf deinem lokalen Computer einzurichten und auszuf√ºhren.

Voraussetzungen
Python 3.10.11 installiert.

Ollama f√ºr dein Betriebssystem installiert.

Ein Hugging Face User Access Token (mit "Read"-Rechten), da langchain-huggingface dies intern f√ºr einige Funktionen verwendet. Diesen Token wirst du als Umgebungsvariable setzen.

1. Ollama Modell herunterladen
√ñffne ein Terminal und lade das gew√ºnschte LLM-Modell herunter. Wir empfehlen gemma:2b f√ºr eine gute Balance zwischen Leistung und Qualit√§t auf CPUs:

ollama run gemma:2b

Warte, bis der Download abgeschlossen ist, und gib dann /bye ein, um die Ollama-Sitzung zu beenden. Stelle sicher, dass der Ollama-Dienst im Hintergrund l√§uft (normalerweise startet er automatisch nach der Installation).

2. Projekt klonen und navigieren
Wenn du das Projekt bereits geklont hast, √ºberspringe diesen Schritt. Andernfalls klone dieses Repository auf deinen lokalen Computer und navigiere in das Projektverzeichnis:

git clone [https://github.com/DEIN_GITHUB_USERNAME/DEIN_REPO_NAME.git](https://github.com/DEIN_GITHUB_USERNAME/DEIN_REPO_NAME.git)
cd DEIN_REPO_NAME

(Ersetze DEIN_GITHUB_USERNAME und DEIN_REPO_NAME durch deine tats√§chlichen Werte)

3. Virtuelles Environment einrichten
Erstelle ein virtuelles Python-Environment und aktiviere es:

python -m venv venv
# Auf Windows PowerShell:
.\venv\Scripts\activate.ps1
# Auf macOS/Linux:
source venv/bin/activate

4. Abh√§ngigkeiten installieren
Installiere alle ben√∂tigten Python-Pakete aus der requirements.txt:

pip install -r requirements.txt

5. Umgebungsvariablen setzen
Bevor du das Backend startest, musst du deinen Hugging Face API Token als Umgebungsvariable setzen. Dies ist notwendig f√ºr die langchain-huggingface Bibliothek, auch wenn wir haupts√§chlich Ollama nutzen.

# Auf Windows PowerShell:
$env:HUGGINGFACEHUB_API_TOKEN="DEIN_HUGGING_FACE_TOKEN"

# Auf macOS/Linux:
export HUGGINGFACEHUB_API_TOKEN="DEIN_HUGGING_FACE_TOKEN"

(Ersetze DEIN_HUGGING_FACE_TOKEN durch deinen tats√§chlichen Token.)

6. Anwendung starten
√ñffne drei separate Terminalfenster und f√ºhre die folgenden Befehle aus:

Terminal 1: Ollama Server (Muss bereits laufen oder gestartet werden)
Stelle sicher, dass der Ollama-Dienst im Hintergrund l√§uft. Wenn er nicht automatisch startet, kannst du ihn manuell starten:

ollama serve

(Dieser Befehl muss w√§hrend der Nutzung des Chatbots aktiv bleiben.)

Terminal 2: FastAPI Backend starten
Navigiere in das backend-Verzeichnis und starte den FastAPI-Server:

cd backend
python -m uvicorn app.main:app --reload

(Warte auf die Meldung INFO: Application startup complete.)

Terminal 3: Gradio Frontend starten
Navigiere zur√ºck in das Hauptprojektverzeichnis und starte die Gradio-Anwendung:

cd .. # Falls du noch im Backend-Ordner bist
python gradio_app.py

(√ñffne dann die angezeigte URL in deinem Webbrowser, normalerweise http://127.0.0.1:7860)

üí° Hinweise zur Performance
Die Geschwindigkeit der Antworten h√§ngt stark von der Leistung deiner CPU und dem verwendeten LLM-Modell ab. F√ºr optimale Leistung auf CPU-basierten Systemen wird die Verwendung von kleineren oder quantisierten Modellen wie gemma:2b empfohlen.

ü§ù Mitwirken
F√ºhle dich frei, Issues zu √∂ffnen oder Pull Requests einzureichen, um dieses Projekt zu verbessern.

üìÑ Lizenz
Dieses Projekt steht unter der MIT-Lizenz.